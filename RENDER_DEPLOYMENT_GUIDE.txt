============================================================
 Render 部署指南 - Steam 數據管道 (v4.0 - 面試準備版)
============================================================

### 簡介

本指南將引導您將 Steam 數據管道專案部署到 Render 平台。
此專案包含兩個核心服務：一個 PostgreSQL 資料庫和一個用於執行爬蟲的背景工人 (Background Worker)。

我們將採用手動部署流程，以確保設定完全符合目前的專業架構。
**重要提示：請忽略專案中的 `render.yaml` 檔案**，因為它對應的是舊的架構，已不再適用。

---

### 前置準備

1.  **確認程式碼已推送**：確保所有最新的程式碼修改都已推送到您的 GitHub repository 的 `main` 分支。
2.  **登入 Render**：打開您的網頁瀏覽器並登入您的 Render 帳號。

---

### 步驟一：建立 PostgreSQL 資料庫

1.  在 Render 主儀表板，點擊右上角的 **New +** 按鈕，然後選擇 **PostgreSQL**。
2.  在設定頁面中，填入以下資訊：
    *   **Name**: `steam-db` (建議使用此名稱以便識別)
    *   **Database**: `steam_data`
    *   **Region**: `Oregon (US West)` (務必選擇與您的背景工人相同的區域，以最小化延遲並降低數據傳輸成本)
    *   **Plan**: `Free`
3.  點擊頁面底部的 **Create Database**。
4.  請耐心等待幾分鐘，直到資料庫的狀態顯示為 **`Available`** (可用)。

---

### 步驟二：建立 Background Worker

1.  回到 Render 主儀表板，點擊 **New +** > **Background Worker**。
2.  連接您的 GitHub 帳號，並選擇您存放此專案的 repository。
    *(注意：這一步至關重要！請務必從「New +」選單中選擇 `Background Worker`，因為服務類型在建立後無法更改。)*
3.  在設定頁面中，填入以下資訊：
    *   **Name**: `steam-scraper-worker` (建議使用此名稱)
    *   **Region**: `Oregon (US West)` (務必選擇與您的資料庫相同的區域)
    *   **Branch**: `main`
    *   **Build Command**: `pip install -r requirements.txt` (Render 會自動偵測 `requirements.txt` 並執行此指令)
    *   **Start Command**: `python runner.py`
    *   **Instance Type**: `Free`
4.  點擊頁面底部的 **Create Background Worker**。
    *(**注意：這是預期行為！** 第一次部署會因缺少必要的環境變數而失敗。我們的流程是先建立服務，再安全地注入配置。)*

---

### 步驟三：設定環境變數 (最關鍵的一步)

1.  部署失敗後，進入剛剛建立的 `steam-scraper-worker` 的儀表板。
2.  點擊左側的 **Environment** 頁籤。
3.  在 "Environment Variables" 區塊，點擊 **Add Environment Variable**，逐一新增所有必要的變數：
    *   **資料庫連線**:
        *   Key: `DATABASE_URL`, Value: (點擊輸入框，從下拉選單選擇 `steam-db (PostgreSQL)` > `Internal Database URL`)
        *   **[面試亮點]** 這裡選擇 `Internal Database URL` 而非 `External` 是基於安全與效能的最佳實踐，因為它能確保服務間的通訊在 Render 的私有網路內進行。
    *   **API 金鑰**:
        *   Key: `STEAM_API_KEY`, Value: `貼上您的 Steam API 金鑰`
        *   Key: `TWITCH_CLIENT_ID`, Value: `貼上您的 Twitch Client ID`
        *   Key: `TWITCH_CLIENT_SECRET`, Value: `貼上您的 Twitch Client Secret`
    *   **代理服務金鑰 (極其重要)**:
        *   Key: `SCRAPERAPI_KEY`, Value: `貼上您從 ScraperAPI 儀表板獲取的 API 金鑰`
    *   **(可選) 效能參數**:
        *   Key: `PYTHON_VERSION`, Value: `3.11` (明確指定 Python 版本，確保部署環境與本地開發一致)
        *   Key: `SCRAPER_BATCH_SIZE`, Value: `20`
            *   **[效能備註]** 此值定義了每次批次處理的遊戲數量。`20` 是一個針對 Render 免費方案 (記憶體有限) 的保守且穩定的設定。較大的值會加快爬取速度，但可能導致記憶體超限而使服務崩潰。
        *   Key: `SCRAPER_CONCURRENCY_LIMIT`, Value: `1`
            *   **[效能備註]** 此值限制了同時發出的網路請求數量。`1` 是最「有禮貌」的設定，能有效避免因請求過快而被目標網站封鎖 (429 錯誤)。
    *   **(可選) 未來升級選項**:
        *   Key: `USE_PREMIUM_PROXY`, Value: `false` (預設為 false。未來若升級代理方案，可設為 true 以啟用住宅 IP。)
    *   **(可選) 排程時間**:
        *   Key: `WORKER_RUN_HOUR_UTC`, Value: `17` (此值定義了每日隨機執行時間窗的**起點**。`17` 代表從 UTC 17:00 開始，即您的時區 UTC+8 的凌晨 1 點。)

4.  點擊頁面底部的 **Save Changes**。

---

### 步驟四：驗證與監控

1.  **自動重新部署**：儲存環境變數後，Render 會自動觸發一次新的部署。
2.  **監控日誌**：進入 `steam-scraper-worker` 的 **Logs** 頁籤。您應該會看到日誌輸出，代表您的爬蟲已經成功啟動並開始工作。
3.  **確認資料寫入**：您可以隨時使用 DBeaver 等工具連接到您的 `steam-db` 資料庫，查詢 `scraping_state` 表來查看最新的進度。
    *(推薦：您可以使用專業的資料庫圖形化工具如 `pgAdmin4` 或 `DBeaver` 來連線。如果您使用 VS Code，也可以安裝 "PostgreSQL" 擴充套件。對於偏好命令列的開發者，`psql` 是最高效的選擇。)*
    *(pgAdmin4 小技巧：如果您在 pgAdmin4 中找不到左側的「Browser」導覽視窗，可以點擊頂端主選單的 `File` -> `Reset Layout...` 來恢復預設版面配置。)*
恭喜您！您已成功部署一個專業、穩健且可恢復的大規模數據抓取任務。

---

### 如何用 Git 更新部署

您的專案已啟用 Render 的自動部署 (Auto-Deploy) 功能。這意味著您不需要在 Render 網站上手動操作來更新您的服務。整個更新流程完全透過 Git 完成。

**核心原理**：每當您將新的程式碼改動推送到您連接的 GitHub repository 的 `main` 分支時，Render 就會自動觸發一次新的部署。

**進階部署控制：使用 Commit 訊息**

為了提供更靈活的部署選項，本專案的背景工人能夠讀取您的 Git Commit 訊息，並根據特定關鍵字調整其啟動行為。

*   **預設行為**：每次部署後，背景工人都會**立即執行一次**完整的爬取任務。
*   **進階控制**：如果您只想更新程式碼，而不希望服務在部署後立即執行任務（例如，您修復了一個小 bug，並希望等待下一個正常的排程時間），您可以在 `git commit` 訊息中加入 `[skip-run]` 關鍵字。

這讓您可以精確控制每一次部署的後續行為。

---

**更新步驟**:

1.  **在本地修改程式碼**:
    在您的專案資料夾中，對任何您想更新的檔案進行修改 (例如 `main.py`, `runner.py` 等)。

2.  **將變更加入 Git 暫存區**:
    打開終端機，切換到您的專案目錄，然後執行：
    ```bash
    git add .
    ```

3.  **提交您的變更**:
    為這次的更新建立一個快照，並附上描述性的訊息。根據您的需求，選擇以下一種場景：

    *   **場景 A：部署後立即執行任務 (預設)**
        這適用於您新增了重要功能，並希望立即驗證效果的情況。
        ```bash
        git commit -m "Feature: 新增遊戲標籤的抓取功能"
        ```

    *   **場景 B：部署後不執行任務，等待下一次排程**
        這適用於您只想更新程式碼（如修復 bug），而不希望觸發一次額外的爬取。
        ```bash
        git commit -m "Fix: 修正了 Twitch API 的解析邏輯 [skip-run]"
        ```

4.  **將變更推送到 GitHub (觸發部署)**:
    執行以下指令，將您的提交上傳到 GitHub：
    ```bash
    git push origin main
    ```

5.  **在 Render 上監控部署**:
    推送完成後，您可以前往 Render 儀表板，進入您的 `steam-scraper-worker` 服務，並在 **Events** 或 **Logs** 頁籤中查看新部署的進度。

---

### 步驟五：驗證數據增長

部署完成後，您的背景工人會持續運行。您可以透過以下方法，使用資料庫客戶端工具來確認資料正在有效地、逐漸地增加。

1.  **使用 `psql` 命令列工具 (推薦)**:
    a.  安裝 PostgreSQL 的 "Command Line Tools"。
    b.  打開終端機，準備貼上您在 Render 儀表板複製的 "External Connection String"。

    > **[!!] 安全警告 (Security Warning) [!!]**
    > 您的連線字串 (Connection String) 包含資料庫密碼，屬於高度敏感資訊。
    > **絕對不要** 將其分享給他人，或提交到 Git 版本控制中。
    > 以下僅為格式範例，請務必使用您自己的連線字串。

    ```bash
    psql "[在此貼上從 Render 儀表板複製的 External Connection String]"
    ```
    *** Windows 使用者提示 ***
    如果您在 Windows 的終端機 (如 Git Bash / MINGW64) 中執行查詢時遇到 `...has no equivalent in encoding "BIG5"` 的錯誤，
    這是因為您的客戶端編碼與資料庫的 UTF-8 編碼不符。
    請在執行 `psql` 連線指令 **之前**，先執行以下指令來設定客戶端編碼：
    `export PGCLIENTENCODING=UTF8`
    為了讓此設定永久生效，您可以將此指令加入您的 shell 設定檔中 (例如 `echo 'export PGCLIENTENCODING=UTF8' >> ~/.bash_profile`)。
    *(提示：在 Windows 上，您可以使用 `cmd` 或 `PowerShell`；在 macOS 或 Linux 上，使用 `Terminal`。貼上指令後按 Enter 鍵。)*
    c.  連線成功後，您就可以執行 SQL 查詢了。

2.  **檢查爬取狀態**:
    執行以下 SQL 查詢，查看爬蟲的鎖定狀態。`is_scraping_active` 的值應該會在爬取時變為 `true`，爬取結束後變為 `false`。
    *(提示: `last_processed_index` 是舊版架構的殘留欄位，可以忽略。)*
    ```sql
    SELECT * FROM scraping_state;
    ```

3.  **確認元數據與時間序列數據數量**:
    執行以下查詢，確認資料表的總記錄數。這兩個數字應該會隨著爬蟲的進度而穩定增加。
    ```sql
    SELECT COUNT(*) FROM games_metadata;
    SELECT COUNT(*) FROM games_timeseries;
    ```

4.  **抽樣檢查資料有效性**:
    隨機抽取幾筆資料，檢查其內容是否符合預期，例如查看最新的時間序列資料。
    ```sql
    SELECT * FROM games_timeseries ORDER BY timestamp DESC LIMIT 5;
    ```

透過定期執行這些查詢，您就可以確信您的數據管道正在健康、穩定地運行。

---

### 下一步：視覺化與分析 (第一階段)

當您確認數據管道已成功部署並開始收集數據後，下一步是執行專案規劃書中的第一階段：**數據驗證與管道監控**。
這個步驟將使用免費的 Looker Studio 工具來快速驗證數據的健康狀況，為後續的 Streamlit 開發打下堅實的基礎。請參閱專案中的 `VISUALIZATION_PLAN.md` 檔案，以獲取完整的視覺化與分析策略。

---

### 常見問題與解決方案 (FAQ)

1.  **Q: 我的數據好幾個小時沒有更新了，怎麼辦？**

    A: 請依照以下步驟排查：
    a.  **檢查日誌**: 前往 `steam-scraper-worker` 服務的 **Logs** 頁籤，查看是否有紅色的錯誤訊息。
    b.  **檢查服務狀態**: 在 **Metrics** 頁籤中，檢查 CPU 和 Memory 使用率是否曾達到 100%，這可能表示服務因資源耗盡而崩潰重啟。
    c.  **檢查資料庫鎖**: 使用資料庫工具查詢 `SELECT * FROM scraping_state;`。如果 `is_scraping_active` 的值為 `true`，但 `last_started_utc` 的時間已經是很久以前，這可能表示上次的任務異常終止。此時，您可以透過手動部署來安全地重啟服務。

2.  **Q: 我想立即手動運行一次爬蟲來測試，該怎麼做？**

    A: 最簡單的方法是透過 Git 觸發一次新的部署，並確保您的 commit 訊息中**不包含** `[skip-run]`。您甚至可以提交一個空的 commit 來觸發：
    ```bash
    git commit --allow-empty -m "Trigger: Manually re-running the scraper"
    git push origin main
    ```

3.  **Q: `[skip-run]` 功能好像沒有生效？**

    A: 請確認以下幾點：
    a.  **語法正確**: 確保關鍵字 `[skip-run]` 完整且包含方括號 `[]`。
    b.  **環境依賴**: 檢查 `Dockerfile` 是否已安裝 `git`。如果未安裝，Python 腳本將無法讀取 commit 訊息。
    c.  **檢查日誌**: 在服務啟動的日誌中，尋找 "偵測到 '[skip-run]'" 或 "無法執行 'git' 指令" 的相關訊息，這能幫助您定位問題。